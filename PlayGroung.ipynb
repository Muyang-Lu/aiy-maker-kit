{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def path(name):\n",
    "    root = os.path.dirname(os.path.realpath(__file__))\n",
    "    return os.path.join(root, 'models', name)\n",
    "\n",
    "# Models\n",
    "FACE_DETECTION_MODEL = path('ssd_mobilenet_v2_face_quant_postprocess_edgetpu.tflite')\n",
    "OBJECT_DETECTION_MODEL = path('ssd_mobilenet_v2_coco_quant_postprocess_edgetpu.tflite')\n",
    "CLASSIFICATION_MODEL = path('tf2_mobilenet_v2_1.0_224_ptq_edgetpu.tflite')\n",
    "CLASSIFICATION_IMPRINTING_MODEL = path('mobilenet_v1_1.0_224_l2norm_quant_edgetpu.tflite')\n",
    "MOVENET_MODEL = path('movenet_single_pose_lightning_ptq_edgetpu.tflite')\n",
    "\n",
    "# Labels\n",
    "CLASSIFICATION_LABELS = path('imagenet_labels.txt')\n",
    "OBJECT_DETECTION_LABELS = path('coco_labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pycoral.utils import edgetpu\n",
    "from pycoral.adapters import detect\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "from pycoral.adapters import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORAL_COLOR = (86, 104, 237)\n",
    "\n",
    "model = \"./example/models/tf2_mobilenet_v2_1.0_224_ptq_edgetpu.tflite\"\n",
    "\n",
    "interpreter = edgetpu.make_interpreter(model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "\n",
    "def get_objects(frame, threshold=0.01):\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    _, scale = common.set_resized_input(interpreter, (width, height),\n",
    "                                        lambda size: cv2.resize(frame, size, fx=0, fy=0, interpolation=cv2.INTER_CUBIC))\n",
    "    interpreter.invoke()\n",
    "    return detect.get_objects(interpreter, threshold, scale)\n",
    "\n",
    "\n",
    "def draw_objects(frame, objs, labels=None, color=CORAL_COLOR, thickness=5):\n",
    "\n",
    "    for obj in objs:\n",
    "        bbox = obj.bbox\n",
    "        cv2.rectangle(frame, (bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax),\n",
    "                      color, thickness)\n",
    "        if labels: \n",
    "            cv2.putText(frame, labels.get(obj.id), (bbox.xmin + thickness, bbox.ymax - thickness),\n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=1, color=CORAL_COLOR, thickness=2)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector:\n",
    "    \"\"\"Performs inferencing with an object detection model.\n",
    "\n",
    "    Args:\n",
    "      model (str): Path to a ``.tflite`` file (compiled for the Edge TPU).\n",
    "        Must be an SSD model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.interpreter = edgetpu.make_interpreter(model)\n",
    "        self.interpreter.allocate_tensors()\n",
    "\n",
    "    def get_objects(self, frame, threshold=0.01):\n",
    "\n",
    "        height, width, _ = frame.shape\n",
    "        _, scale = common.set_resized_input(self.interpreter, (width, height),\n",
    "                                            lambda size: cv2.resize(frame, size,\n",
    "                                                                    fx=0, fy=0,\n",
    "                                                                    interpolation=cv2.INTER_CUBIC))\n",
    "        self.interpreter.invoke()\n",
    "        return detect.get_objects(self.interpreter, threshold, scale)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aiymakerkit import vision\n",
    "import models\n",
    "\n",
    "detector = vision.Detector(models.FACE_DETECTION_MODEL)\n",
    "\n",
    "for frame in vision.get_frames():\n",
    "    faces = detector.get_objects(frame, threshold=0.1)\n",
    "    vision.draw_objects(frame, faces)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
